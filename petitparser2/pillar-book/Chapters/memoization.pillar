${inputFile:Chapters/navigation.pillar}$

!Manual Memoization

@@note Please note that the ==WebGrammar== offered to download in the *Abstract Syntax Tree>AST.pillar* chapter contains already optimized version of an ==element== rule. In this chapter we suppose that ==element== definition looks like defined in *Extracting the Structure>chapter3.pillar* chapter, i.e. it looks like this:b
[[[
WebGrammar>>element
	^ (elOpen, elContent, elClose)
]]]


In the previous chapter *Optimizations>optimizations.pillar* we have greatly improved the performance of our parser. 
So let's parse the real sources.
The home page of we *wikipedia>https://wikipedia.org*, *github>https://github.com*, *facebook>https://facebook.com* and *google>https://google.com* can be parsed invoking this commands:

[[[
sources := PP2Sources current htmlSourcesAll.
parser := WebParser new optimize.
sources collect: [ :s |
	parser parse: s.
]
]]]

Unfortunately, this still takes too much time even though we applied the automated optimizations. 
Can we do something, that the optimizations can't? 
Usually, hard to say. 
It depends on the nature of a grammar and input to be parsed.
But in this case, we can really improve the performance. (obviously, we wouldn't write this chapter otherwise).

It is a good time to check the events morph of a debug result. 
Events morph shows timeline of parser invocations (dot) at a given position (x axis) in a given time (y axis).
Inspect the result of the following command again and switch to 'Events'' view.

[[[
WebParser new optimize debug: input.
]]]


+>figures/optimized-trace.png|width=100%+

On a screenshot we see only a part of the story (the part that fits into a window).
The whole story is that parsing progress pretty fast towards the end of input (this is the good part),  but later one can notice (when scrolling down) that parser jumps to the beginning of input and starts again. 
Over and over again (this is the bad part).

@@note Please not that for long inputs the ''Event'' tab shows only a beginning of the input and first few thousands of invocations.

Even though optimizations in PetitParser2 work reasonably well, not all of them can be applied automatically. 
Some grammars do a lot of backtracking and automated optimizations can reduce only part of it.
If I ask you why is the backtracking happening you would probably have a hard time to figure this out${footnote: actually, it was not easy for the author of this text to pinpoint the root cause as well}$.
So does have the code trying to apply the optimizations, so we have to do this manually.


Luckily, there are tools to help us. 
For convenience of visualization, we use compact and simplified input.

[[[
compact := '
<!a>
<h>
<m foo>
<m e>
<b>
Lorem ipsum donor sit amet
</b>
</h>
'.
WebParser new optimize debug: compact.
]]]


+>figures/short-trace.png|width=100%+


!!Searching for the cause


In the events morph we see how does the parser backtrack, over and over. 
We have to do a bit of detective work to figure out when and why. 
Let us navigate through the parsing until the HTML body (represented by the ==b== element in the ''compact'' input) is parsed:



+>figures/optimized-debug-element.png|width=100%+


Now switch to the traces tab:

+>figures/optimized-trace-element.png|width=100%+



The yellow rectangle highlights 872 underlying invocations of the ==element== rule. 
The dark red rectangle highlights another invocations of the same ==element== rule that started at the same position. 
And we see there are quite a few of them. 
In general, one does not want to see repeating dark-red rectangles, and the more of them or the higher these dark-red rectangles are, the worse. 
It simply means redundant computations. 
Remember, each pixel on the y-axis is a parser invocation.

@@todo JK:how the heck context fit this story???

Why do these redundant invocations happen? 
Because of the unclosed HTML ==meta== elements (represented by ==m== in the ''compact'' input). 
The parser sees meta, starts an element, parses the content of meta, including the body element. 
But it does not find the closing meta. 
So it returns before the meta element, skips the meta part as a water and continues parsing. 
This means it re-parses the body element again. 
The more ==meta== elements, the worse.

If yo are watchful, you might have noticed a line behind a dark-red bar. 
This line is created by ==PP2Cache== which immediately returns the result of a ==body== element and prevents the whole execution.
But because it can remember only last result, it can prevent only half of the executions.

!!Fixing the Cause
There is a solution to this, called memoization. 
Memoization is a technique to remember all the results for all the positions. 
We already saw memoizations applied to the bounded seas.
Now we should suggest PetitParser2 to apply the same trick for the element rule.
We do this by adding a ==memoize== keyword to the ==element== rule.

[[[
WebGrammar>>element
	^ (elOpen, elContent, elClose)
		memoize;
		yourself
]]]

As we have already discussed, ==memoize== is specified manually. 
Suitable positions for memoization are hard to detect automatically and memoizations are relatively expensive. 
Mamoizations should not be used superfluously


Let us check the result now:

[[[
WebParser new optimize debug: compact.
]]]


+>figures/memoized-trace.png|width=100%+


This is the result we want to see. 
Even though the parser still backtracks because of the nature of the grammar (i.e. we see two big backtrackings, one for each ==meta== in the input), the html body itself is not reparsed over and over again and the precious time is saved.


You can try to parse some big sources as well. 

[[[
sources := PP2Sources current htmlSourcesAll.
parser := WebParser new optimize.
sources collect: [ :s |
	parser parse: s.
]
]]]

!!Conclusion
This is the last chapter about the html parser.
We started with a prototype in the playground in the *first chapter>chapter1.pillar* and end up with a full-fledged and tested parser that is able to parser real sources and being tolerant to malformed inputs in this chapter. 

In each step, we wrote only a code that describes what we are interested in
and we got feedback from the early stages of the development.
This is in contrast with a traditional parser development, where a grammar implementor has to specify most or a complete grammar before obtaining useful results.

PetitParser allows a grammar implementor to use such a agile approach to parsing thanks to 
(i) bounded seas, which allows for the tolerant parsing, and (ii) automated optimizations, which significantly improve performance of a removing the burden from the grammar implementor.

!!!Suggestions and ideas?
The parser we implemented has a reasonable performance considering the simplicity of the grammar and effort we put into specifying it.
Of course, PetitParser allows you to specify even faster parser, but at a cost of higher implementation effort.
We discuss optimizations of a high-performance grammar on the concrete example of a *Smalltalk grammar>smalltalkOptimizations.pillar* which can reach the performance of a hand-written parser.

If you know how to improve the performance even more, please share the ideas or contribute!

!!!Sources
The sources of the html parser can be found here: 
- *==WebGrammar==>../WebGrammar.st* 
- *==WebGrammarTest==>../WebGrammarTest.st*
- *==WebParser==>../WebParser.st* 
- *==WebParserTest==>../WebParserTest.st*
- *==WebElement==>../WebElement.st*
- *==JavascriptElement==>../JavascriptElement.st*
- *==HtmlElement==>../HtmlElement.st*
- *==UnknownText==>../UnknownText.st*

The source for this tutorial is also part of the PetitParser2 package.

The image with the VMs for Linux, Mac OS and Windows can be found here:
@@todo image

${inputFile:Chapters/toc.pillar}$
