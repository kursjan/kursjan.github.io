!Manual Memoization

We can try to parse the real sources. 
The home page of we wikipedia, github, facebook and google can be parsed invoking this commands:

[[[
sources := PP2Sources current htmlSourcesAll.
parser := WebParser new optimize.
sources collect: [ :s |
	parser parse: s.
]
]]]

Unfortunately, this still takes too much time even though we applied the automated optimizations. 
Can we do something, that the optimizations canâ€™t? 

Actually we can. 
It is a good time to check the events morph of a debug result. Events morph shows timeline of parser invocations (dot) at a given position (x axis) in a given order (y axis).


Inspect a result of the following command again:

[[[
WebParser new optimize debug: input.
]]]


+>figures/optimized-trace.png|width=100%+


On a screenshot we see only a part of the story: parsing progress pretty fast towards the end of input. 
This is the good part. 
But when scrolling down, one can notice that at some point, parser backtracks and returns to the beginning. 
Over and over again.
And this is the bad part.


Even though optimizations in PetitParser2 work reasonably good, not all of them can be applied automatically. 
Some grammars do a lot of backtracking which is hard to detect automatically. 
If I ask you why is the backtracking happening you would probably have a hard time to figure this out. ${footnote:actually, it was not easy for the author of this text to pinpoint the root cause as well}


Luckily, there are tools to help you. Because of the format of this blog, we will more compact and simplified input.

[[[
compact := '
<!a>
<h>
<m foo>
<m e>
<b>
Lorem ipsum donor sit amet
</b>
</h>
'.
WebParser new optimize debug: compact.
]]]


+>figures/short-trace.png|width=100%+


!!Searching for the cause


In the events morph we see how is the parser backtracking over and over. 
We have to play a bit of detective to figure out when and why. 
Let us navigate through the parsing until the body (the b element) is parsed:



+>figures/optimized-debug-element.png|width=100%+


Now switch to the traces tab:

+>figures/optimized-trace-element.png|width=100%+



The yellow rectangle highlights 872 underlying invocations of the element rule. 
The dark red rectangle highlights another invocations of the same element rule that started at the same position. 
And we see there are quite a few of them. 
In general, one does not want to see repeating and high dark rectangles. 
It simply means redundant computations. 
Remember, each pixel on the y-axis is a parser innvocation.


Why do these redundant invocations happen? 
Because of the unclosed meta elements (named only m in the compact input). 
The parser sees meta, starts an element, parses the content of meta, including the body element. 
But it does not find the closing meta. 
So it returns before the meta element, skips the meta part as a water and continues parsing. Which effectively means it re-parses the body element again. 
The ==PP2Cache== does not help much, because the last invocation of an element rule is the failed meta element. 
Thus the result of the body element is forgotten.

!!Fixing the Cause
There is a solution to this, called memoization, which remembers all the results for all the positions. 
We already saw memoizations applied to the bounded seas.
Now we can suggest to PetitParser2 to try the same trick for the element rule:

[[[
WebGrammar>>element
	^ (elOpen, elContent, elClose)
		memoize;
		yourself
]]]

We have to specify memoize manually, because memoization are hard to detect automatically. 
One has to understand, how is the input structured and how does the parser works to apply it efficiently. 
And it is better not to use memoizations superfluously, because they are relatively expensive and not worth in most of the cases.


Let us check the result now:

[[[
WebParser new optimize debug: compact.
]]]


+>figures/memoized-trace.png|width=100%+


This is the result we want to see. 
Even though the parser still backtracks because of the nature of the grammar (i.e. meta rules, we see two big backtrackings, one for each meta in the input), the html body itself is not reparsed over and over again and the precious time is saved.


You can try to parse some big sources as well. 
You will get the results in a reasonable time (considering the simplicity of the grammar and effort we put into specifying it):

[[[
sources := PP2Sources current htmlSourcesAll.
parser := WebParser new optimize.
sources collect: [ :s |
	parser parse: s.
]
]]]

If you know how to improve the performance even more, please share the ideas or contribute!


The sources can be found here: 
@@todo sources

The image with the VMs for Linux, Mac OS and Windows can be found here:
@@todo image
